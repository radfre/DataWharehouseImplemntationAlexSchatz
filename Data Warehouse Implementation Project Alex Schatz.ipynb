{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "pppvcsd5lbfb7qc7tckg",
   "authorId": "6787178515854",
   "authorName": "ASCHATZ",
   "authorEmail": "aschatz@hawk.iit.edu",
   "sessionId": "329c2a23-7db6-4aae-95bf-f70c63adc879",
   "lastEditTime": 1742018010149
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "STEP1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark.window import Window\nsession = get_active_session()\n\n\ndf_amazon = session.table(\"hulu_MOVIES_TV\")\ndf_disney = session.table(\"disney_MOVIES_TV\")\ndf_hulu = session.table(\"HULU_MOVIES_TV\")\ndf_netflix = session.table(\"netflix_MOVIES_TV\")\n\ndf_industry = session.table(\"INDUSTRY_MOVIES\")\ndf_IMBD = session.table(\"IMDB_MOVIE_REVIEWS\")\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "69848991-b6e7-4ec0-aa40-745181dc5ef9",
   "metadata": {
    "language": "python",
    "name": "STEP2",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\n# Select only the required columns for df_amazon\ndf_amazon_with_columns = df_amazon \\\n    .with_column(\"amazon\",F.lit(None)) \\\n    .with_column(\"disney\", F.lit(None)) \\\n    .with_column(\"hulu\", F.lit(None)) \\\n    .with_column(\"netflix\", F.lit(None)) \\\n    .select(\"TYPE\", \"TITLE\", \"DIRECTOR\", \"CAST\", \"COUNTRY\", \"DATE_ADDED\", \n            \"RELEASE_YEAR\", \"RATING\", \"DURATION\", \"LISTED_IN\", \"DESCRIPTION\")\n\n# Select only the required columns for df_disney\ndf_disney_with_columns = df_disney \\\n    .with_column(\"amazon\", F.lit(None)) \\\n    .with_column(\"disney\", F.lit(None)) \\\n    .with_column(\"hulu\", F.lit(None)) \\\n    .with_column(\"netflix\", F.lit(None)) \\\n    .select(\"TYPE\", \"TITLE\", \"DIRECTOR\", \"CAST\", \"COUNTRY\", \"DATE_ADDED\", \n            \"RELEASE_YEAR\", \"RATING\", \"DURATION\", \"LISTED_IN\", \"DESCRIPTION\")\n\n# Select only the required columns for df_hulu\ndf_hulu_with_columns = df_hulu \\\n    .with_column(\"amazon\", F.lit(None)) \\\n    .with_column(\"disney\", F.lit(None)) \\\n    .with_column(\"hulu\",F.lit(None)) \\\n    .with_column(\"netflix\", F.lit(None)) \\\n    .select(\"TYPE\", \"TITLE\", \"DIRECTOR\", \"CAST\", \"COUNTRY\", \"DATE_ADDED\", \n            \"RELEASE_YEAR\", \"RATING\", \"DURATION\", \"LISTED_IN\", \"DESCRIPTION\")\n\n# Select only the required columns for df_netflix\ndf_netflix_with_columns = df_netflix \\\n    .with_column(\"amazon\", F.lit(None)) \\\n    .with_column(\"disney\", F.lit(None)) \\\n    .with_column(\"hulu\", F.lit(None)) \\\n    .with_column(\"netflix\", F.lit(None)) \\\n    .select(\"TYPE\", \"TITLE\", \"DIRECTOR\", \"CAST\", \"COUNTRY\", \"DATE_ADDED\", \n            \"RELEASE_YEAR\", \"RATING\", \"DURATION\", \"LISTED_IN\", \"DESCRIPTION\")\n\n\n# Union all the DataFrames into df_streaming with the required columns\ndf_streaming = df_amazon_with_columns \\\n    .union(df_disney_with_columns) \\\n    .union(df_hulu_with_columns) \\\n    .union(df_netflix_with_columns)\n\n# drop duplicate titles\ndf_streaming = df_streaming.groupBy(\"TITLE\").agg(\n    F.coalesce(F.max(\"TYPE\"), F.lit(None)).alias(\"TYPE\"),\n    F.coalesce(F.max(\"DIRECTOR\"), F.lit(None)).alias(\"DIRECTOR\"),\n    F.coalesce(F.max(\"CAST\"), F.lit(None)).alias(\"CAST\"),\n    F.coalesce(F.max(\"COUNTRY\"), F.lit(None)).alias(\"COUNTRY\"),\n    F.coalesce(F.max(\"DATE_ADDED\"), F.lit(None)).alias(\"DATE_ADDED\"),\n    F.coalesce(F.max(\"RELEASE_YEAR\"), F.lit(None)).alias(\"RELEASE_YEAR\"),\n    F.coalesce(F.max(\"RATING\"), F.lit(None)).alias(\"RATING\"),\n    F.coalesce(F.max(\"DURATION\"), F.lit(None)).alias(\"DURATION\"),\n    F.coalesce(F.max(\"LISTED_IN\"), F.lit(None)).alias(\"LISTED_IN\"),\n    F.coalesce(F.max(\"DESCRIPTION\"), F.lit(None)).alias(\"DESCRIPTION\")\n)\n\n# Add primary key (row number)\ndf_streaming = df_streaming.with_column(\"streaming_key\", F.row_number().over(Window.order_by(F.lit(1))))\n\n# Save to table (assuming Snowflake environment is set up)\ndf_streaming.write \\\n    .mode(\"overwrite\") \\\n    .save_as_table(\"STREAMING_MOVIES_TV\")\n\ndf_streaming.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44e5cb15-35bd-4636-85d7-4b80a0bb5c72",
   "metadata": {
    "language": "python",
    "name": "STEP3",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Calculate missing values before cleaning\nmissing_values = df_streaming.select([\n    F.sum(F.when(F.col(col).is_null(), 1).otherwise(0)).alias(col)\n    for col in df_streaming.columns\n])\nmissing_values.show()\n\n# Fill missing 'rating' with mode and 'country' with 'Unknown'\nmode_value = df_streaming.groupBy('RATING').count().orderBy(F.desc('RATING')).first()['RATING']\ndf_streaming = df_streaming.fillna({'RATING': mode_value, 'COUNTRY': 'Unknown'})\n\n# Fill missing 'director' and 'cast' with 'Not available'\ndf_streaming = df_streaming.fillna({'DIRECTOR': 'Not available', 'CAST': 'Not available'})\n\n# Fix 'date_added': Convert 'DATE_ADDED' to date, if null set to January 1st of 'RELEASE_YEAR'\ndf_streaming = df_streaming.with_column(\n    \"date_added\",\n    F.coalesce(\n        F.to_date(df_streaming[\"DATE_ADDED\"], \"MMMM dd, yyyy\"),\n        F.to_date(F.concat(df_streaming[\"RELEASE_YEAR\"], F.lit(\"-01-01\")))\n    )\n)\n\ndf_streaming.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01cdff8b-bb6b-47bf-a9c8-2f6c8c4629d0",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "684280d0-e63a-4c6c-9fa8-3d1f66782528",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "\ndf_industry.show()\ndf_IMBD.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "107af328-553d-4539-b913-03a17486535e",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4bac15a6-3a99-44ac-9f7e-e21f69bfc400",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "# Assuming you already have the Snowpark session initialized and the dataframes loaded\n\n# Perform the join operation on the NAME column\ndf_matches = df_IMBD.join(df_industry, on=\"NAME\", how=\"inner\")\n\n\n# Order the results by IMDB_ID (assuming IMDB_ID exists in df_IMBD)\ndf_matches = df_matches.orderBy(\"IMBD_ID\")\n\n# Show the first 10 rows where there is a match\ndf_matches.show(10)\n\n# Count the total number of matching records\ntotal_matches = df_matches.count()\n\n# Count the total number of records in each dataframe\ntotal_records_IMBD = df_IMBD.count()\ntotal_records_industry = df_industry.count()\n\n# Output the results\nprint(f\"Total matching records: {total_matches}\")\nprint(f\"Total records in df_IMBD: {total_records_IMBD}\")\nprint(f\"Total records in df_industry: {total_records_industry}\")\n",
   "execution_count": null
  }
 ]
}